# -*- coding: utf-8 -*-
"""Advanced Time Series Forecasting with Seq2Seq and Attention.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_WBz7qLcguEXCIo9puCrArnZLkOecJ7t
"""

import pandas as pd

url = "https://raw.githubusercontent.com/jbrownlee/Datasets/master/airline-passengers.csv"
data = pd.read_csv(url)

data.head()

data = pd.concat([data]*4, ignore_index=True)
print(len(data))

import matplotlib.pyplot as plt

plt.figure(figsize=(10,4))
plt.plot(data["Passengers"])
plt.title("Airline Passengers Time Series")
plt.xlabel("Time")
plt.ylabel("Passengers")
plt.show()

import numpy as np

values = data["Passengers"].values.reshape(-1, 1)
values.shape

train_size = int(len(values) * 0.7)
val_size = int(len(values) * 0.15)

train = values[:train_size]
val = values[train_size:train_size + val_size]
test = values[train_size + val_size:]

len(train), len(val), len(test)

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
train_scaled = scaler.fit_transform(train)
val_scaled = scaler.transform(val)
test_scaled = scaler.transform(test)

def create_sequences(data, lookback=30, horizon=5):
    X, y = [], []
    for i in range(len(data) - lookback - horizon):
        X.append(data[i:i+lookback])
        y.append(data[i+lookback:i+lookback+horizon])
    return np.array(X), np.array(y)

LOOKBACK = 30
HORIZON = 5

X_train, y_train = create_sequences(train_scaled, LOOKBACK, HORIZON)
X_val, y_val = create_sequences(val_scaled, LOOKBACK, HORIZON)
X_test, y_test = create_sequences(test_scaled, LOOKBACK, HORIZON)

X_train.shape, y_train.shape

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
device

X_train_t = torch.tensor(X_train, dtype=torch.float32).to(device)
y_train_t = torch.tensor(y_train, dtype=torch.float32).to(device)

X_val_t = torch.tensor(X_val, dtype=torch.float32).to(device)
y_val_t = torch.tensor(y_val, dtype=torch.float32).to(device)

X_test_t = torch.tensor(X_test, dtype=torch.float32).to(device)
y_test_t = torch.tensor(y_test, dtype=torch.float32).to(device)

train_loader = DataLoader(
    TensorDataset(X_train_t, y_train_t),
    batch_size=32,
    shuffle=True
)

val_loader = DataLoader(
    TensorDataset(X_val_t, y_val_t),
    batch_size=32,
    shuffle=False
)

class LSTMBaseline(nn.Module):
    def __init__(self, input_size=1, hidden_size=64, output_size=5):
        super().__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        out, _ = self.lstm(x)
        out = out[:, -1, :]          # last time step
        out = self.fc(out)
        return out.unsqueeze(-1)     # (batch, horizon, 1)

model = LSTMBaseline(output_size=HORIZON).to(device)
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

EPOCHS = 20

for epoch in range(EPOCHS):
    model.train()
    train_loss = 0

    for xb, yb in train_loader:
        optimizer.zero_grad()
        preds = model(xb)
        loss = criterion(preds, yb)
        loss.backward()
        optimizer.step()
        train_loss += loss.item()

    val_loss = 0
    model.eval()
    with torch.no_grad():
        for xb, yb in val_loader:
            preds = model(xb)
            val_loss += criterion(preds, yb).item()

    print(f"Epoch {epoch+1}/{EPOCHS} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}")

from sklearn.metrics import mean_squared_error, mean_absolute_error
import numpy as np

model.eval()
with torch.no_grad():
    baseline_preds = model(X_test_t).cpu().numpy()
    baseline_true = y_test_t.cpu().numpy()

# reshape
baseline_preds = baseline_preds.reshape(-1)
baseline_true = baseline_true.reshape(-1)

rmse_baseline = np.sqrt(mean_squared_error(baseline_true, baseline_preds))
mae_baseline = mean_absolute_error(baseline_true, baseline_preds)
mape_baseline = np.mean(np.abs((baseline_true - baseline_preds) / baseline_true)) * 100

rmse_baseline, mae_baseline, mape_baseline

class LuongAttention(nn.Module):
    def __init__(self, hidden_size):
        super().__init__()
        self.attn = nn.Linear(hidden_size, hidden_size)

    def forward(self, decoder_hidden, encoder_outputs):
        scores = torch.bmm(
            encoder_outputs,
            decoder_hidden.unsqueeze(2)
        ).squeeze(2)

        weights = torch.softmax(scores, dim=1)
        context = torch.bmm(weights.unsqueeze(1), encoder_outputs)
        return context.squeeze(1)

class Seq2SeqAttention(nn.Module):
    def __init__(self, input_size=1, hidden_size=64, horizon=5):
        super().__init__()
        self.encoder = nn.LSTM(input_size, hidden_size, batch_first=True)
        self.decoder = nn.LSTM(input_size, hidden_size, batch_first=True)
        self.attention = LuongAttention(hidden_size)
        self.fc = nn.Linear(hidden_size * 2, 1)
        self.horizon = horizon

    def forward(self, x):
        encoder_outputs, (h, c) = self.encoder(x)

        decoder_input = x[:, -1:, :]
        outputs = []

        for _ in range(self.horizon):
            dec_out, (h, c) = self.decoder(decoder_input, (h, c))
            context = self.attention(h[-1], encoder_outputs)
            combined = torch.cat((dec_out.squeeze(1), context), dim=1)
            pred = self.fc(combined)
            outputs.append(pred.unsqueeze(1))
            decoder_input = pred.unsqueeze(1)

        return torch.cat(outputs, dim=1)

attn_model = Seq2SeqAttention(horizon=HORIZON).to(device)
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(attn_model.parameters(), lr=0.001)

EPOCHS = 25

for epoch in range(EPOCHS):
    attn_model.train()
    train_loss = 0

    for xb, yb in train_loader:
        optimizer.zero_grad()
        preds = attn_model(xb)
        loss = criterion(preds, yb)
        loss.backward()
        optimizer.step()
        train_loss += loss.item()

    print(f"Epoch {epoch+1}/{EPOCHS} | Train Loss: {train_loss:.4f}")

attn_model.eval()
with torch.no_grad():
    attn_preds = attn_model(X_test_t).cpu().numpy()
    attn_true = y_test_t.cpu().numpy()

attn_preds = attn_preds.reshape(-1)
attn_true = attn_true.reshape(-1)

rmse_attn = np.sqrt(mean_squared_error(attn_true, attn_preds))
mae_attn = mean_absolute_error(attn_true, attn_preds)
mape_attn = np.mean(np.abs((attn_true - attn_preds) / attn_true)) * 100

rmse_attn, mae_attn, mape_attn

import pandas as pd

results = pd.DataFrame({
    "Model": ["LSTM Baseline", "Seq2Seq + Attention"],
    "RMSE": [rmse_baseline, rmse_attn],
    "MAE": [mae_baseline, mae_attn],
    "MAPE": [mape_baseline, mape_attn]
})

results